{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A Convolutional Network implementation example using TensorFlow library.\n",
    "This example is using the MNIST database of handwritten digits\n",
    "(http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics as mt\n",
    "from sklearn import utils as ut\n",
    "import csv\n",
    "import numpy as np\n",
    "import os.path as path\n",
    "from PIL import Image\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "featuresDir = './data/SampleFoodClassifier_Norm_100'\n",
    "\n",
    "with open('./sample_food_no_food.csv') as f:\n",
    "    food_no_food = [{k: v for k, v in row.items()}\n",
    "        for row in csv.DictReader(f, skipinitialspace=True)]\n",
    "    \n",
    "\n",
    "data_ids = [element['id'] for element in food_no_food]\n",
    "\n",
    "labels = [int(element['is_food']) for element in food_no_food]\n",
    "#data = [rgb2gray(np.array(Image.open(path.join(featuresDir, element)))) for element in data_ids]\n",
    "data = [np.array(Image.open(path.join(featuresDir, element))) for element in data_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width, Height and channels: 100 100 3\n"
     ]
    }
   ],
   "source": [
    "# Split training data in a train set and a test set. The test set will containt 20% of the total\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(data, labels, test_size=.25, random_state=6)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_size = 30\n",
    "training_split= 0.8 #split into training_split 1 and (1-training_split) 0\n",
    "training_iters = 15\n",
    "\n",
    "training_size_1=np.floor(training_split*training_size)\n",
    "training_size_0=training_size-training_size_1\n",
    "\n",
    "# Network Parameters\n",
    "w, h, channels = data[0].shape\n",
    "print \"Width, Height and channels:\", w, h, channels\n",
    "n_classes = len(set(y_train))\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# print 'Input vector size', n_input, 'train shape', np.array(x_train).shape , 'number of classes', n_classes\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, w, h, channels])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    print \"PLEASE MODIFY WD1 TO\", conv2.get_shape().as_list()[1], \"*\",conv2.get_shape().as_list()[2], \"*64\"\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLEASE MODIFY WD1 TO 25 * 25 *64\n"
     ]
    }
   ],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, channels, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([25*25*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "y_p = tf.argmax(pred, 1)\n",
    "\n",
    "def label_class(x):\n",
    "    for i in range(0,len(x)):\n",
    "        print i, \":\", x[i]\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:28: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "y_train_temp = []\n",
    "\n",
    "for element in y_train:\n",
    "    temp = [0]*len(set(y_train))\n",
    "    temp[element] = 1\n",
    "    y_train_temp.append(temp)\n",
    "    \n",
    "y_train = np.reshape(y_train_temp,(len(y_train_temp), -1))\n",
    "\n",
    "y_test_temp = []\n",
    "\n",
    "for element in y_test:\n",
    "    temp = [0]*len(set(y_test))\n",
    "    temp[element] = 1\n",
    "    y_test_temp.append(temp)\n",
    "    \n",
    "y_test = np.reshape(y_test_temp,(len(y_test_temp), -1))\n",
    "\n",
    "# Proportional sampling from both classes, get features for 0 and 1 each\n",
    "y_help=np.array([el[1] for el in y_train])\n",
    "y_index_0 = np.where(y_help==0)[0]\n",
    "y_index_1 = np.where(y_help==1)[0]\n",
    "\n",
    "x_0 = [x_train[index] for index in y_index_0]\n",
    "x_1 = [x_train[index] for index in y_index_1]\n",
    "\n",
    "#y batch looks always the same for if using proportional sampling\n",
    "iy= np.vstack(([[1,0]]*training_size_0,[[0,1]]*training_size_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/utils/__init__.py:211: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  indices = indices[:max_n_samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Minibatch Loss= 28280588.000000, Training Accuracy= 0.80000\n",
      "Iter 1, Minibatch Loss= 44994724.000000, Training Accuracy= 0.80000\n",
      "Iter 2, Minibatch Loss= 54719420.000000, Training Accuracy= 0.80000\n",
      "Iter 3, Minibatch Loss= 45295788.000000, Training Accuracy= 0.80000\n",
      "Iter 4, Minibatch Loss= 42607360.000000, Training Accuracy= 0.80000\n",
      "Iter 5, Minibatch Loss= 48841088.000000, Training Accuracy= 0.80000\n",
      "Iter 6, Minibatch Loss= 25986486.000000, Training Accuracy= 0.80000\n",
      "Iter 7, Minibatch Loss= 17499726.000000, Training Accuracy= 0.86667\n",
      "Iter 8, Minibatch Loss= 16236339.000000, Training Accuracy= 0.80000\n",
      "Iter 9, Minibatch Loss= 9595893.000000, Training Accuracy= 0.76667\n",
      "Iter 10, Minibatch Loss= 27922866.000000, Training Accuracy= 0.63333\n",
      "Iter 11, Minibatch Loss= 28605290.000000, Training Accuracy= 0.53333\n",
      "Iter 12, Minibatch Loss= 6781874.000000, Training Accuracy= 0.76667\n",
      "Iter 13, Minibatch Loss= 15184340.000000, Training Accuracy= 0.63333\n",
      "Iter 14, Minibatch Loss= 19891266.000000, Training Accuracy= 0.76667\n",
      "Optimization Finished!\n",
      "Partial testing accuracy: 1.0\n",
      "Partial testing accuracy: 0.949999988079\n",
      "Partial testing accuracy: 0.799999992053\n",
      "Partial testing accuracy: 0.774999991059\n",
      "Partial testing accuracy: 0.779999995232\n",
      "Partial testing accuracy: 0.766666660706\n",
      "Partial testing accuracy: 0.757142850331\n",
      "Partial testing accuracy: 0.76249999553\n",
      "Partial testing accuracy: 0.766666664018\n",
      "Partial testing accuracy: 0.779999995232\n",
      "Partial testing accuracy: 0.790909084407\n",
      "Partial testing accuracy: 0.774999996026\n",
      "Partial testing accuracy: 0.776923074172\n",
      "Partial testing accuracy: 0.792857140303\n",
      "Partial testing accuracy: 0.793333331744\n",
      "Partial testing accuracy: 0.79999999702\n",
      "Partial testing accuracy: 0.799999997896\n",
      "Partial testing accuracy: 0.794444441795\n",
      "Partial testing accuracy: 0.784210525061\n",
      "Partial testing accuracy: 0.789999997616\n",
      "Partial testing accuracy: 0.795238091832\n",
      "Partial testing accuracy: 0.790909087116\n",
      "Partial testing accuracy: 0.791304344716\n",
      "Partial testing accuracy: 0.79999999702\n",
      "Partial testing accuracy: 0.795999996662\n",
      "Partial testing accuracy: 0.796153843403\n",
      "Partial testing accuracy: 0.799999996468\n",
      "Partial testing accuracy: 0.79999999702\n",
      "Partial testing accuracy: 0.799999997534\n",
      "Partial testing accuracy: 0.786666664481\n",
      "Partial testing accuracy: 0.790322577761\n",
      "Partial testing accuracy: 0.790624997579\n",
      "Partial testing accuracy: 0.790909088922\n",
      "Validation accuracy: 0.790909088922\n",
      "Precision for each class:\n",
      "0 : 0.44\n",
      "1 : 0.854545454545\n",
      "Recall for each class:\n",
      "0 : 0.354838709677\n",
      "1 : 0.893536121673\n",
      "F1_score for each class:\n",
      "0 : 0.392857142857\n",
      "1 : 0.873605947955\n",
      "confusion_matrix\n",
      "[[ 22  40]\n",
      " [ 28 235]]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Keep training until reach max iterations\n",
    "    for epoch in range(training_iters):\n",
    "        # Fit training using batch data\n",
    "        x_batch_0 = ut.shuffle(x_0, n_samples=training_size_0, random_state=epoch)\n",
    "        x_batch_1 = ut.shuffle(x_1, n_samples=training_size_1, random_state=epoch)\n",
    "        ix = x_batch_0+x_batch_1\n",
    "        \n",
    "        #ix = ut.shuffle(x_train, n_samples=training_size, random_state=epoch)\n",
    "        #iy = ut.shuffle(y_train, n_samples=training_size, random_state=epoch)\n",
    "        \n",
    "        sess.run(optimizer, feed_dict={x: ix, y: iy, keep_prob: 1.})\n",
    "        # Compute average loss\n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict={x: ix, y: iy, keep_prob: 1.})\n",
    "        # Display logs per epoch step\n",
    "        print \"Iter \" + str(epoch) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc)\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    runs = 0\n",
    "    acc = 0.\n",
    "    y_pred = []\n",
    "    for i in range(0, len(y_test), 10):\n",
    "        if i+10 < len(y_test):\n",
    "            val_accuracy, y_pred_i = sess.run([accuracy, y_p], feed_dict={x: x_test[i:i+10], y: y_test[i:i+10], keep_prob: 1.})\n",
    "            acc += val_accuracy\n",
    "            y_pred.extend(y_pred_i)\n",
    "        else:\n",
    "            val_accuracy, y_pred_i = sess.run([accuracy, y_p], feed_dict={x: x_test[i:], y: y_test[i:], keep_prob: 1.})\n",
    "            acc += val_accuracy\n",
    "            y_pred.extend(y_pred_i)\n",
    "        runs += 1\n",
    "        print \"Partial testing accuracy:\", acc/runs\n",
    "    \n",
    "    #metrics\n",
    "    print \"Validation accuracy:\", acc/runs\n",
    "    y_true = np.argmax(y_test,1)\n",
    "    print \"Precision for each class:\"\n",
    "    label_class(mt.precision_score(y_true, y_pred, average=None))\n",
    "    print \"Recall for each class:\"\n",
    "    label_class(mt.recall_score(y_true, y_pred, average=None))\n",
    "    print \"F1_score for each class:\"\n",
    "    label_class(mt.f1_score(y_true, y_pred, average=None))\n",
    "    print \"confusion_matrix\"\n",
    "    print mt.confusion_matrix(y_true, y_pred)\n",
    "    fpr, tpr, tresholds = mt.roc_curve(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
