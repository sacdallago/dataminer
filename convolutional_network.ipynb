{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A Convolutional Network implementation example using TensorFlow library.\n",
    "This example is using the MNIST database of handwritten digits\n",
    "(http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics as mt\n",
    "from sklearn import utils as ut\n",
    "import csv\n",
    "import numpy as np\n",
    "import os.path as path\n",
    "from PIL import Image\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "featuresDir = './data/SampleFoodClassifier_Norm_100'\n",
    "\n",
    "with open('./sample_food_no_food.csv') as f:\n",
    "    food_no_food = [{k: v for k, v in row.items()}\n",
    "        for row in csv.DictReader(f, skipinitialspace=True)]\n",
    "    \n",
    "\n",
    "data_ids = [element['id'] for element in food_no_food]\n",
    "\n",
    "labels = [int(element['is_food']) for element in food_no_food]\n",
    "#data = [rgb2gray(np.array(Image.open(path.join(featuresDir, element)))) for element in data_ids]\n",
    "data = [np.array(Image.open(path.join(featuresDir, element))) for element in data_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width, Height and channels: 100 100 3\n"
     ]
    }
   ],
   "source": [
    "seed=ut.resample(np.linspace(1,1000,1000,dtype=int), n_samples=1)[0]\n",
    "\n",
    "# Split training data in a train set and a test set. The test set will containt 20% of the total\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(data, labels, test_size=.25,random_state=seed)\n",
    "\n",
    "# Parameters\n",
    "#learning_rate = 0.001\n",
    "learning_rate_start= 0.0001\n",
    "training_size = 20\n",
    "training_split= 0.8 #split into training_split 1 or 0 and (1-training_split) 0 \n",
    "training_iters_max = 100\n",
    "training_freq= 2       #how often is each picture trained (on average!)\n",
    "sdev= 0.01 #for variable initialization the higher the stddev, the more iterations are needed\n",
    "#stddev too small leads to stationary variables (don't take it smaller than 0.01!)!\n",
    "\n",
    "training_size_1=np.floor(training_split*training_size).astype(int)\n",
    "training_size_0=(training_size-training_size_1).astype(int)\n",
    "\n",
    "# Network Parameters\n",
    "w, h, channels = data[0].shape\n",
    "print \"Width, Height and channels:\", w, h, channels\n",
    "n_classes = len(set(y_train))\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# print 'Input vector size', n_input, 'train shape', np.array(x_train).shape , 'number of classes', n_classes\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, w, h, channels])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    print \"PLEASE MODIFY WD1 TO\", conv2.get_shape().as_list()[1], \"*\",conv2.get_shape().as_list()[2], \"*64\"\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLEASE MODIFY WD1 TO 25 * 25 *64\n"
     ]
    }
   ],
   "source": [
    "# Store layers weight & bias\n",
    "\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.truncated_normal([5, 5, channels, 32], stddev=sdev, seed=1)),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=sdev, seed=2)),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.truncated_normal([25*25*64, 1024], stddev=sdev, seed=3)),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.truncated_normal([1024, n_classes], stddev=sdev, seed=4))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.truncated_normal([32], stddev=sdev, seed=5)),\n",
    "    'bc2': tf.Variable(tf.truncated_normal([64], stddev=sdev, seed=6)),\n",
    "    'bd1': tf.Variable(tf.truncated_normal([1024], stddev=sdev, seed=7)),\n",
    "    'out': tf.Variable(tf.truncated_normal([n_classes], stddev=sdev, seed=8))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "\n",
    "#optimizer without adapted learning_rate\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "#optimizer with adapted learning_rate\n",
    "step = tf.Variable(0, trainable=False)\n",
    "rate = tf.train.exponential_decay(learning_rate_start, step, 1, 0.9999)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(rate).minimize(cost, global_step=step)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "y_p = tf.argmax(pred, 1)\n",
    "\n",
    "def label_class(x):\n",
    "    for i in range(0,len(x)):\n",
    "        print i, \":\", x[i]\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_temp = []\n",
    "\n",
    "for element in y_train:\n",
    "    temp = [0]*len(set(y_train))\n",
    "    temp[element] = 1\n",
    "    y_train_temp.append(temp)\n",
    "    \n",
    "y_train = np.reshape(y_train_temp,(len(y_train_temp), -1))\n",
    "\n",
    "y_test_temp = []\n",
    "\n",
    "for element in y_test:\n",
    "    temp = [0]*len(set(y_test))\n",
    "    temp[element] = 1\n",
    "    y_test_temp.append(temp)\n",
    "    \n",
    "y_test = np.reshape(y_test_temp,(len(y_test_temp), -1))\n",
    "\n",
    "# Proportional sampling from both classes, get features for 0 and 1 each\n",
    "y_help=np.array([el[1] for el in y_train])\n",
    "y_index_0 = np.where(y_help==0)[0]\n",
    "y_index_1 = np.where(y_help==1)[0]\n",
    "\n",
    "x_0 = [x_train[index] for index in y_index_0]\n",
    "x_1 = [x_train[index] for index in y_index_1]\n",
    "\n",
    "#y batch looks always the same for if using proportional sampling\n",
    "iy=np.vstack(([[1,0]]*training_size_0,[[0,1]]*training_size_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Minibatch Loss= 3.859382, Training Accuracy= 0.80000\n",
      "Iter 1, Minibatch Loss= 2.409132, Training Accuracy= 0.80000\n",
      "Iter 2, Minibatch Loss= 1.755600, Training Accuracy= 0.80000\n",
      "Iter 3, Minibatch Loss= 0.600566, Training Accuracy= 0.80000\n",
      "Iter 4, Minibatch Loss= 0.651980, Training Accuracy= 0.65000\n",
      "Iter 5, Minibatch Loss= 0.801312, Training Accuracy= 0.30000\n",
      "Iter 6, Minibatch Loss= 0.738230, Training Accuracy= 0.25000\n",
      "Iter 7, Minibatch Loss= 0.626504, Training Accuracy= 0.70000\n",
      "Iter 8, Minibatch Loss= 0.542264, Training Accuracy= 0.85000\n",
      "Iter 9, Minibatch Loss= 0.496183, Training Accuracy= 0.80000\n",
      "Iter 10, Minibatch Loss= 0.490651, Training Accuracy= 0.80000\n",
      "Iter 11, Minibatch Loss= 0.451452, Training Accuracy= 0.80000\n",
      "Iter 12, Minibatch Loss= 0.419875, Training Accuracy= 0.80000\n",
      "Iter 13, Minibatch Loss= 0.443054, Training Accuracy= 0.80000\n",
      "Iter 14, Minibatch Loss= 0.437133, Training Accuracy= 0.80000\n",
      "Iter 15, Minibatch Loss= 0.397308, Training Accuracy= 0.80000\n",
      "Iter 16, Minibatch Loss= 0.458078, Training Accuracy= 0.80000\n",
      "Iter 17, Minibatch Loss= 0.443666, Training Accuracy= 0.80000\n",
      "Iter 18, Minibatch Loss= 0.413303, Training Accuracy= 0.80000\n",
      "Iter 19, Minibatch Loss= 0.469964, Training Accuracy= 0.80000\n",
      "Iter 20, Minibatch Loss= 0.411771, Training Accuracy= 0.80000\n",
      "Iter 21, Minibatch Loss= 0.414441, Training Accuracy= 0.80000\n",
      "Iter 22, Minibatch Loss= 0.459550, Training Accuracy= 0.80000\n",
      "Iter 23, Minibatch Loss= 0.433971, Training Accuracy= 0.80000\n",
      "Iter 24, Minibatch Loss= 0.373733, Training Accuracy= 0.80000\n",
      "Iter 25, Minibatch Loss= 0.322924, Training Accuracy= 0.80000\n",
      "Iter 26, Minibatch Loss= 0.426641, Training Accuracy= 0.80000\n",
      "Iter 27, Minibatch Loss= 0.317022, Training Accuracy= 0.80000\n",
      "Iter 28, Minibatch Loss= 0.360274, Training Accuracy= 0.80000\n",
      "Iter 29, Minibatch Loss= 0.385363, Training Accuracy= 0.80000\n",
      "Iter 30, Minibatch Loss= 0.331134, Training Accuracy= 0.85000\n",
      "Iter 31, Minibatch Loss= 0.388925, Training Accuracy= 0.80000\n",
      "Iter 32, Minibatch Loss= 0.368698, Training Accuracy= 0.80000\n",
      "Iter 33, Minibatch Loss= 0.270814, Training Accuracy= 0.80000\n",
      "Iter 34, Minibatch Loss= 0.255939, Training Accuracy= 0.85000\n",
      "Iter 35, Minibatch Loss= 0.342376, Training Accuracy= 0.85000\n",
      "Iter 36, Minibatch Loss= 0.485246, Training Accuracy= 0.85000\n",
      "Iter 37, Minibatch Loss= 0.470434, Training Accuracy= 0.85000\n",
      "Iter 38, Minibatch Loss= 0.254860, Training Accuracy= 0.90000\n",
      "Iter 39, Minibatch Loss= 0.345293, Training Accuracy= 0.85000\n",
      "Iter 40, Minibatch Loss= 0.439821, Training Accuracy= 0.85000\n",
      "Iter 41, Minibatch Loss= 0.403276, Training Accuracy= 0.85000\n",
      "Iter 42, Minibatch Loss= 0.366693, Training Accuracy= 0.85000\n",
      "Iter 43, Minibatch Loss= 0.314276, Training Accuracy= 0.90000\n",
      "Iter 44, Minibatch Loss= 0.232392, Training Accuracy= 0.85000\n",
      "Iter 45, Minibatch Loss= 0.228430, Training Accuracy= 0.80000\n",
      "Iter 46, Minibatch Loss= 0.344303, Training Accuracy= 0.85000\n",
      "Iter 47, Minibatch Loss= 0.277080, Training Accuracy= 0.85000\n",
      "Iter 48, Minibatch Loss= 0.259349, Training Accuracy= 0.85000\n",
      "Iter 49, Minibatch Loss= 0.352311, Training Accuracy= 0.80000\n",
      "Iter 50, Minibatch Loss= 0.254530, Training Accuracy= 0.95000\n",
      "Iter 51, Minibatch Loss= 0.369085, Training Accuracy= 0.90000\n",
      "Iter 52, Minibatch Loss= 0.403947, Training Accuracy= 0.95000\n",
      "Iter 53, Minibatch Loss= 0.353254, Training Accuracy= 0.80000\n",
      "Iter 54, Minibatch Loss= 0.379052, Training Accuracy= 0.90000\n",
      "Iter 55, Minibatch Loss= 0.220268, Training Accuracy= 0.90000\n",
      "Iter 56, Minibatch Loss= 0.333767, Training Accuracy= 0.85000\n",
      "Iter 57, Minibatch Loss= 0.237079, Training Accuracy= 0.85000\n",
      "Iter 58, Minibatch Loss= 0.357475, Training Accuracy= 0.80000\n",
      "Iter 59, Minibatch Loss= 0.205997, Training Accuracy= 0.95000\n",
      "Iter 60, Minibatch Loss= 0.312316, Training Accuracy= 0.85000\n",
      "Iter 61, Minibatch Loss= 0.200914, Training Accuracy= 1.00000\n",
      "Iter 62, Minibatch Loss= 0.220558, Training Accuracy= 0.90000\n",
      "Iter 63, Minibatch Loss= 0.469810, Training Accuracy= 0.80000\n",
      "Iter 64, Minibatch Loss= 0.202850, Training Accuracy= 0.95000\n",
      "Iter 65, Minibatch Loss= 0.404236, Training Accuracy= 0.90000\n",
      "Iter 66, Minibatch Loss= 0.204618, Training Accuracy= 0.95000\n",
      "Iter 67, Minibatch Loss= 0.184147, Training Accuracy= 1.00000\n",
      "Iter 68, Minibatch Loss= 0.343217, Training Accuracy= 0.90000\n",
      "Iter 69, Minibatch Loss= 0.215700, Training Accuracy= 0.80000\n",
      "Iter 70, Minibatch Loss= 0.173432, Training Accuracy= 0.95000\n",
      "Iter 71, Minibatch Loss= 0.233145, Training Accuracy= 0.95000\n",
      "Iter 72, Minibatch Loss= 0.206415, Training Accuracy= 0.85000\n",
      "Iter 73, Minibatch Loss= 0.322324, Training Accuracy= 0.80000\n",
      "Iter 74, Minibatch Loss= 0.372087, Training Accuracy= 0.80000\n",
      "Iter 75, Minibatch Loss= 0.189781, Training Accuracy= 0.95000\n",
      "Iter 76, Minibatch Loss= 0.170332, Training Accuracy= 0.95000\n",
      "Iter 77, Minibatch Loss= 0.257496, Training Accuracy= 0.90000\n",
      "Iter 78, Minibatch Loss= 0.214830, Training Accuracy= 0.90000\n",
      "Iter 79, Minibatch Loss= 0.231408, Training Accuracy= 0.90000\n",
      "Iter 80, Minibatch Loss= 0.176445, Training Accuracy= 0.95000\n",
      "Iter 81, Minibatch Loss= 0.325918, Training Accuracy= 0.85000\n",
      "Iter 82, Minibatch Loss= 0.126766, Training Accuracy= 0.95000\n",
      "Iter 83, Minibatch Loss= 0.121577, Training Accuracy= 0.95000\n",
      "Iter 84, Minibatch Loss= 0.108880, Training Accuracy= 0.95000\n",
      "Iter 85, Minibatch Loss= 0.397618, Training Accuracy= 0.85000\n",
      "Iter 86, Minibatch Loss= 0.541023, Training Accuracy= 0.75000\n",
      "Iter 87, Minibatch Loss= 0.313851, Training Accuracy= 0.85000\n",
      "Iter 88, Minibatch Loss= 0.152440, Training Accuracy= 0.95000\n",
      "Iter 89, Minibatch Loss= 0.211628, Training Accuracy= 0.85000\n",
      "Iter 90, Minibatch Loss= 0.201417, Training Accuracy= 0.90000\n",
      "Iter 91, Minibatch Loss= 0.111252, Training Accuracy= 0.95000\n",
      "Iter 92, Minibatch Loss= 0.573840, Training Accuracy= 0.80000\n",
      "Iter 93, Minibatch Loss= 0.288464, Training Accuracy= 0.85000\n",
      "Optimization Finished!\n",
      "Partial testing accuracy: 1.0\n",
      "Partial testing accuracy: 0.949999988079\n",
      "Partial testing accuracy: 0.966666658719\n",
      "Partial testing accuracy: 0.84999999404\n",
      "Partial testing accuracy: 0.779999995232\n",
      "Partial testing accuracy: 0.783333331347\n",
      "Partial testing accuracy: 0.799999994891\n",
      "Partial testing accuracy: 0.79999999702\n",
      "Partial testing accuracy: 0.788888884915\n",
      "Partial testing accuracy: 0.779999995232\n",
      "Partial testing accuracy: 0.736363633112\n",
      "Partial testing accuracy: 0.749999995033\n",
      "Partial testing accuracy: 0.761538455119\n",
      "Partial testing accuracy: 0.771428563765\n",
      "Partial testing accuracy: 0.786666659514\n",
      "Partial testing accuracy: 0.799999993294\n",
      "Partial testing accuracy: 0.79999999439\n",
      "Partial testing accuracy: 0.799999995364\n",
      "Partial testing accuracy: 0.799999996235\n",
      "Partial testing accuracy: 0.804999995232\n",
      "Partial testing accuracy: 0.799999994891\n",
      "Partial testing accuracy: 0.804545448585\n",
      "Partial testing accuracy: 0.804347820904\n",
      "Partial testing accuracy: 0.812499995033\n",
      "Partial testing accuracy: 0.811999995708\n",
      "Partial testing accuracy: 0.81153845787\n",
      "Partial testing accuracy: 0.818518514986\n",
      "Partial testing accuracy: 0.814285710454\n",
      "Partial testing accuracy: 0.81379310016\n",
      "Partial testing accuracy: 0.813333330552\n",
      "Partial testing accuracy: 0.809677416278\n",
      "Partial testing accuracy: 0.812499996275\n",
      "Partial testing accuracy: 0.81212120887\n",
      "Validation accuracy: 0.81212120887\n",
      "Precision for each class:\n",
      "0 : 0.725490196078\n",
      "1 : 0.828467153285\n",
      "Recall for each class:\n",
      "0 : 0.440476190476\n",
      "1 : 0.941908713693\n",
      "F1_score for each class:\n",
      "0 : 0.548148148148\n",
      "1 : 0.881553398058\n",
      "confusion_matrix\n",
      "[[ 37  47]\n",
      " [ 14 227]]\n",
      "seed used for splitting: 736\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    dummy_0 = len(x_0)\n",
    "    dummy_1 = len(x_1)\n",
    "    \n",
    "    # Keep training until reach max iterations\n",
    "    for epoch in range(training_iters_max):\n",
    "        \n",
    "        if (dummy_0 < training_size_0 or dummy_1 < training_size_1):\n",
    "            break\n",
    "            \n",
    "        # Fit training using batch data first training_size_0 data are 0 rest random\n",
    "        x_batch_0 = ut.resample(x_0, n_samples=training_size_0,replace=False, random_state=seed+epoch)\n",
    "        x_batch_1 = ut.resample(x_1, n_samples=training_size_1,replace=False, random_state=seed+epoch)\n",
    "        ix = x_batch_0+x_batch_1 #is for concatenating the lists, no addition done here\n",
    "        \n",
    "        dummy_0 = dummy_0 - training_size_0/training_freq\n",
    "        dummy_1 = dummy_1 - training_size_1/training_freq\n",
    "        \n",
    "        #ix = ut.shuffle(x_train, n_samples=training_size, random_state=epoch)\n",
    "        #iy = ut.shuffle(y_train, n_samples=training_size, random_state=epoch)\n",
    "\n",
    "        sess.run(optimizer, feed_dict={x: ix, y: iy, keep_prob: 1.})\n",
    "        \n",
    "        # Compute average loss\n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict={x: ix, y: iy, keep_prob: 1.})\n",
    "        # Display logs per epoch step\n",
    "        print \"Iter \" + str(epoch) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc)\n",
    "    print \"Optimization Finished!\"\n",
    "        \n",
    "    runs = 0\n",
    "    acc = 0.\n",
    "    y_pred = []\n",
    "    for i in range(0, len(y_test), 10):\n",
    "        if i+10 < len(y_test):\n",
    "            val_accuracy, y_pred_i = sess.run([accuracy, y_p], feed_dict={x: x_test[i:i+10], y: y_test[i:i+10], keep_prob: 1.})\n",
    "            acc += val_accuracy\n",
    "            y_pred.extend(y_pred_i)\n",
    "        else:\n",
    "            val_accuracy, y_pred_i = sess.run([accuracy, y_p], feed_dict={x: x_test[i:], y: y_test[i:], keep_prob: 1.})\n",
    "            acc += val_accuracy\n",
    "            y_pred.extend(y_pred_i)\n",
    "        runs += 1\n",
    "        print \"Partial testing accuracy:\", acc/runs\n",
    "    \n",
    "    #metrics\n",
    "    print \"Validation accuracy:\", acc/runs\n",
    "    y_true = np.argmax(y_test,1)\n",
    "    print \"Precision for each class:\"\n",
    "    label_class(mt.precision_score(y_true, y_pred, average=None))\n",
    "    print \"Recall for each class:\"\n",
    "    label_class(mt.recall_score(y_true, y_pred, average=None))\n",
    "    print \"F1_score for each class:\"\n",
    "    label_class(mt.f1_score(y_true, y_pred, average=None))\n",
    "    print \"confusion_matrix:\"\n",
    "    print mt.confusion_matrix(y_true, y_pred)\n",
    "    fpr, tpr, tresholds = mt.roc_curve(y_true, y_pred)\n",
    "    print \"seed used for splitting:\", seed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
