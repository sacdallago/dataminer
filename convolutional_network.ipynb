{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A Convolutional Network implementation example using TensorFlow library.\n",
    "This example is using the MNIST database of handwritten digits\n",
    "(http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics as mt\n",
    "from sklearn import utils as ut\n",
    "import csv\n",
    "import numpy as np\n",
    "import os.path as path\n",
    "from PIL import Image\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "featuresDir = './data/SampleFoodClassifier_Norm_100'\n",
    "\n",
    "with open('./sample_food_no_food.csv') as f:\n",
    "    food_no_food = [{k: v for k, v in row.items()}\n",
    "        for row in csv.DictReader(f, skipinitialspace=True)]\n",
    "    \n",
    "\n",
    "data_ids = [element['id'] for element in food_no_food]\n",
    "\n",
    "labels = [int(element['is_food']) for element in food_no_food]\n",
    "#data = [rgb2gray(np.array(Image.open(path.join(featuresDir, element)))) for element in data_ids]\n",
    "data = [np.array(Image.open(path.join(featuresDir, element))) for element in data_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width, Height and channels: 100 100 3\n"
     ]
    }
   ],
   "source": [
    "# Split training data in a train set and a test set. The test set will containt 20% of the total\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(data, labels, test_size=.25, random_state=6)\n",
    "\n",
    "# Parameters\n",
    "#learning_rate = 0.001\n",
    "learning_rate_start= 0.0001\n",
    "training_size = 20\n",
    "training_split= 0.8 #split into training_split 1 or 0 and (1-training_split) 0 \n",
    "training_iters = 50\n",
    "sdev= 0.01 #for variable initialization the higher the stddev, the more iterations are needed\n",
    "#stddev too small leads to stationary variables (don't take it smaller than 0.01!)!\n",
    "\n",
    "training_size_1=np.floor(training_split*training_size).astype(int)\n",
    "training_size_0=(training_size-training_size_1).astype(int)\n",
    "\n",
    "# Network Parameters\n",
    "w, h, channels = data[0].shape\n",
    "print \"Width, Height and channels:\", w, h, channels\n",
    "n_classes = len(set(y_train))\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# print 'Input vector size', n_input, 'train shape', np.array(x_train).shape , 'number of classes', n_classes\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, w, h, channels])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    print \"PLEASE MODIFY WD1 TO\", conv2.get_shape().as_list()[1], \"*\",conv2.get_shape().as_list()[2], \"*64\"\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLEASE MODIFY WD1 TO 25 * 25 *64\n"
     ]
    }
   ],
   "source": [
    "# Store layers weight & bias\n",
    "\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.truncated_normal([5, 5, channels, 32], stddev=sdev, seed=1)),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=sdev, seed=2)),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.truncated_normal([25*25*64, 1024], stddev=sdev, seed=3)),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.truncated_normal([1024, n_classes], stddev=sdev, seed=4))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.truncated_normal([32], stddev=sdev, seed=5)),\n",
    "    'bc2': tf.Variable(tf.truncated_normal([64], stddev=sdev, seed=6)),\n",
    "    'bd1': tf.Variable(tf.truncated_normal([1024], stddev=sdev, seed=7)),\n",
    "    'out': tf.Variable(tf.truncated_normal([n_classes], stddev=sdev, seed=8))\n",
    "}\n",
    "\n",
    "\n",
    "#weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "#    'wc1': tf.Variable(tf.random_normal([5, 5, channels, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "#    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "#    'wd1': tf.Variable(tf.random_normal([25*25*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "#    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "#}\n",
    "\n",
    "#biases = {\n",
    "#    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "#    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "#    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "#    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "#}\n",
    "\n",
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "\n",
    "#optimizer without adapted learning_rate\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "#optimizer with adapted learning_rate\n",
    "step = tf.Variable(0, trainable=False)\n",
    "rate = tf.train.exponential_decay(learning_rate_start, step, 1, 0.9999)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(rate).minimize(cost, global_step=step)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "y_p = tf.argmax(pred, 1)\n",
    "\n",
    "def label_class(x):\n",
    "    for i in range(0,len(x)):\n",
    "        print i, \":\", x[i]\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_temp = []\n",
    "\n",
    "for element in y_train:\n",
    "    temp = [0]*len(set(y_train))\n",
    "    temp[element] = 1\n",
    "    y_train_temp.append(temp)\n",
    "    \n",
    "y_train = np.reshape(y_train_temp,(len(y_train_temp), -1))\n",
    "\n",
    "y_test_temp = []\n",
    "\n",
    "for element in y_test:\n",
    "    temp = [0]*len(set(y_test))\n",
    "    temp[element] = 1\n",
    "    y_test_temp.append(temp)\n",
    "    \n",
    "y_test = np.reshape(y_test_temp,(len(y_test_temp), -1))\n",
    "\n",
    "# Proportional sampling from both classes, get features for 0 and 1 each\n",
    "y_help=np.array([el[1] for el in y_train])\n",
    "y_index_0 = np.where(y_help==0)[0]\n",
    "y_index_1 = np.where(y_help==1)[0]\n",
    "\n",
    "x_0 = [x_train[index] for index in y_index_0]\n",
    "x_1 = [x_train[index] for index in y_index_1]\n",
    "\n",
    "#y batch looks always the same for if using proportional sampling\n",
    "#y_batch_0= np.vstack(([[1,0]]*training_size_0))\n",
    "iy=np.vstack(([[1,0]]*training_size_0,[[0,1]]*training_size_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Minibatch Loss= 3.776740, Training Accuracy= 0.80000\n",
      "Iter 1, Minibatch Loss= 2.491016, Training Accuracy= 0.80000\n",
      "Iter 2, Minibatch Loss= 1.551401, Training Accuracy= 0.80000\n",
      "Iter 3, Minibatch Loss= 0.486538, Training Accuracy= 0.80000\n",
      "Iter 4, Minibatch Loss= 0.566614, Training Accuracy= 0.70000\n",
      "Iter 5, Minibatch Loss= 0.816923, Training Accuracy= 0.35000\n",
      "Iter 6, Minibatch Loss= 0.636984, Training Accuracy= 0.75000\n",
      "Iter 7, Minibatch Loss= 0.506538, Training Accuracy= 0.80000\n",
      "Iter 8, Minibatch Loss= 0.468608, Training Accuracy= 0.80000\n",
      "Iter 9, Minibatch Loss= 0.414656, Training Accuracy= 0.80000\n",
      "Iter 10, Minibatch Loss= 0.454085, Training Accuracy= 0.80000\n",
      "Iter 11, Minibatch Loss= 0.499937, Training Accuracy= 0.80000\n",
      "Iter 12, Minibatch Loss= 0.455447, Training Accuracy= 0.80000\n",
      "Iter 13, Minibatch Loss= 0.496924, Training Accuracy= 0.80000\n",
      "Iter 14, Minibatch Loss= 0.505688, Training Accuracy= 0.80000\n",
      "Iter 15, Minibatch Loss= 0.407959, Training Accuracy= 0.80000\n",
      "Iter 16, Minibatch Loss= 0.396142, Training Accuracy= 0.80000\n",
      "Iter 17, Minibatch Loss= 0.390670, Training Accuracy= 0.80000\n",
      "Iter 18, Minibatch Loss= 0.407568, Training Accuracy= 0.80000\n",
      "Iter 19, Minibatch Loss= 0.365354, Training Accuracy= 0.80000\n",
      "Iter 20, Minibatch Loss= 0.392694, Training Accuracy= 0.80000\n",
      "Iter 21, Minibatch Loss= 0.384347, Training Accuracy= 0.80000\n",
      "Iter 22, Minibatch Loss= 0.446643, Training Accuracy= 0.80000\n",
      "Iter 23, Minibatch Loss= 0.372404, Training Accuracy= 0.85000\n",
      "Iter 24, Minibatch Loss= 0.544038, Training Accuracy= 0.80000\n",
      "Iter 25, Minibatch Loss= 0.451755, Training Accuracy= 0.80000\n",
      "Iter 26, Minibatch Loss= 0.377238, Training Accuracy= 0.80000\n",
      "Iter 27, Minibatch Loss= 0.366466, Training Accuracy= 0.80000\n",
      "Iter 28, Minibatch Loss= 0.405023, Training Accuracy= 0.80000\n",
      "Iter 29, Minibatch Loss= 0.390998, Training Accuracy= 0.80000\n",
      "Iter 30, Minibatch Loss= 0.372660, Training Accuracy= 0.80000\n",
      "Iter 31, Minibatch Loss= 0.334109, Training Accuracy= 0.85000\n",
      "Iter 32, Minibatch Loss= 0.312780, Training Accuracy= 0.95000\n",
      "Iter 33, Minibatch Loss= 0.429465, Training Accuracy= 0.85000\n",
      "Iter 34, Minibatch Loss= 0.385118, Training Accuracy= 0.80000\n",
      "Iter 35, Minibatch Loss= 0.318138, Training Accuracy= 0.90000\n",
      "Iter 36, Minibatch Loss= 0.241866, Training Accuracy= 0.90000\n",
      "Iter 37, Minibatch Loss= 0.376724, Training Accuracy= 0.80000\n",
      "Iter 38, Minibatch Loss= 0.265991, Training Accuracy= 0.85000\n",
      "Iter 39, Minibatch Loss= 0.368829, Training Accuracy= 0.75000\n",
      "Iter 40, Minibatch Loss= 0.295358, Training Accuracy= 0.85000\n",
      "Iter 41, Minibatch Loss= 0.305574, Training Accuracy= 0.95000\n",
      "Iter 42, Minibatch Loss= 0.410336, Training Accuracy= 0.85000\n",
      "Iter 43, Minibatch Loss= 0.300344, Training Accuracy= 0.85000\n",
      "Iter 44, Minibatch Loss= 0.301749, Training Accuracy= 0.80000\n",
      "Iter 45, Minibatch Loss= 0.457073, Training Accuracy= 0.70000\n",
      "Iter 46, Minibatch Loss= 0.206895, Training Accuracy= 0.90000\n",
      "Iter 47, Minibatch Loss= 0.342607, Training Accuracy= 0.90000\n",
      "Iter 48, Minibatch Loss= 0.457413, Training Accuracy= 0.75000\n",
      "Iter 49, Minibatch Loss= 0.282120, Training Accuracy= 0.85000\n",
      "Optimization Finished!\n",
      "Partial testing accuracy: 1.0\n",
      "Partial testing accuracy: 0.84999999404\n",
      "Partial testing accuracy: 0.799999992053\n",
      "Partial testing accuracy: 0.79999999702\n",
      "Partial testing accuracy: 0.8\n",
      "Partial testing accuracy: 0.816666662693\n",
      "Partial testing accuracy: 0.799999994891\n",
      "Partial testing accuracy: 0.77499999851\n",
      "Partial testing accuracy: 0.777777777778\n",
      "Partial testing accuracy: 0.780000001192\n",
      "Partial testing accuracy: 0.800000001084\n",
      "Partial testing accuracy: 0.791666666667\n",
      "Partial testing accuracy: 0.792307693225\n",
      "Partial testing accuracy: 0.807142857994\n",
      "Partial testing accuracy: 0.813333332539\n",
      "Partial testing accuracy: 0.8125\n",
      "Partial testing accuracy: 0.817647057421\n",
      "Partial testing accuracy: 0.822222219573\n",
      "Partial testing accuracy: 0.821052629697\n",
      "Partial testing accuracy: 0.829999998212\n",
      "Partial testing accuracy: 0.828571427436\n",
      "Partial testing accuracy: 0.83636363528\n",
      "Partial testing accuracy: 0.834782608177\n",
      "Partial testing accuracy: 0.84166666617\n",
      "Partial testing accuracy: 0.843999998569\n",
      "Partial testing accuracy: 0.846153843861\n",
      "Partial testing accuracy: 0.851851849644\n",
      "Partial testing accuracy: 0.857142855014\n",
      "Partial testing accuracy: 0.858620686778\n",
      "Partial testing accuracy: 0.853333330154\n",
      "Partial testing accuracy: 0.854838705832\n",
      "Partial testing accuracy: 0.853124996647\n",
      "Partial testing accuracy: 0.851515148625\n",
      "Validation accuracy: 0.851515148625\n",
      "Precision for each class:\n",
      "0 : 0.609375\n",
      "1 : 0.911877394636\n",
      "Recall for each class:\n",
      "0 : 0.629032258065\n",
      "1 : 0.904942965779\n",
      "F1_score for each class:\n",
      "0 : 0.619047619048\n",
      "1 : 0.908396946565\n",
      "confusion_matrix\n",
      "[[ 39  23]\n",
      " [ 25 238]]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Keep training until reach max iterations\n",
    "    for epoch in range(training_iters):\n",
    "        \n",
    "        # Fit training using batch data first training_size_0 data are 0 rest random\n",
    "        x_batch_0 = ut.shuffle(x_0, n_samples=training_size_0, random_state=epoch)\n",
    "        x_batch_1 = ut.shuffle(x_1, n_samples=training_size_1, random_state=2*epoch)\n",
    "        ix = x_batch_0+x_batch_1 #is for concatenating the lists, no addition done here\n",
    "        \n",
    "        #ix = ut.shuffle(x_train, n_samples=training_size, random_state=epoch)\n",
    "        #iy = ut.shuffle(y_train, n_samples=training_size, random_state=epoch)\n",
    "        \n",
    "        #y_batch = ut.shuffle(y_train, n_samples=training_size_1, random_state=2*epoch)\n",
    "        #iy = np.vstack((y_batch_0,y_batch))        \n",
    "        \n",
    "        sess.run(optimizer, feed_dict={x: ix, y: iy, keep_prob: 1.})\n",
    "        \n",
    "        # Compute average loss\n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict={x: ix, y: iy, keep_prob: 1.})\n",
    "        # Display logs per epoch step\n",
    "        print \"Iter \" + str(epoch) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc)\n",
    "    print \"Optimization Finished!\"\n",
    "        \n",
    "    runs = 0\n",
    "    acc = 0.\n",
    "    y_pred = []\n",
    "    for i in range(0, len(y_test), 10):\n",
    "        if i+10 < len(y_test):\n",
    "            val_accuracy, y_pred_i = sess.run([accuracy, y_p], feed_dict={x: x_test[i:i+10], y: y_test[i:i+10], keep_prob: 1.})\n",
    "            acc += val_accuracy\n",
    "            y_pred.extend(y_pred_i)\n",
    "        else:\n",
    "            val_accuracy, y_pred_i = sess.run([accuracy, y_p], feed_dict={x: x_test[i:], y: y_test[i:], keep_prob: 1.})\n",
    "            acc += val_accuracy\n",
    "            y_pred.extend(y_pred_i)\n",
    "        runs += 1\n",
    "        print \"Partial testing accuracy:\", acc/runs\n",
    "    \n",
    "    #metrics\n",
    "    print \"Validation accuracy:\", acc/runs\n",
    "    y_true = np.argmax(y_test,1)\n",
    "    print \"Precision for each class:\"\n",
    "    label_class(mt.precision_score(y_true, y_pred, average=None))\n",
    "    print \"Recall for each class:\"\n",
    "    label_class(mt.recall_score(y_true, y_pred, average=None))\n",
    "    print \"F1_score for each class:\"\n",
    "    label_class(mt.f1_score(y_true, y_pred, average=None))\n",
    "    print \"confusion_matrix\"\n",
    "    print mt.confusion_matrix(y_true, y_pred)\n",
    "    fpr, tpr, tresholds = mt.roc_curve(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
