{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''\n",
    "A Convolutional Network implementation example using TensorFlow library.\n",
    "This example is using the MNIST database of handwritten digits\n",
    "(http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics as mt\n",
    "from sklearn import utils as ut\n",
    "import gc\n",
    "import csv\n",
    "import numpy as np\n",
    "import os.path as path\n",
    "from os import listdir\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./photo_to_levels.csv') as f:\n",
    "    food_to_label = []\n",
    "    for row in csv.DictReader(f, skipinitialspace=True):\n",
    "        element = {}\n",
    "        for k, v in row.items():\n",
    "            if k == \"id\":\n",
    "                element['id'] = str(v) + \".jpg\"\n",
    "            elif k == \"labels\":\n",
    "                labels_raw = np.array(str(v).split(' '))\n",
    "                labels = [0] * 9\n",
    "                labels_int = []\n",
    "                try:\n",
    "                    for lb in labels_raw:\n",
    "                        labels[int(str(lb))] = 1\n",
    "                        labels_int.append(int(lb))\n",
    "                except ValueError:\n",
    "                    i=0 #dummy thing to prevent the print spam below\n",
    "                    #print \"Failure with value\", lb, \"labels lenght\", len(labels_raw), \"content:\", v\n",
    "                element['labels'] = labels\n",
    "                element['labels_raw'] = labels_int\n",
    "            else :\n",
    "                print \"No idea what you just passed!\"\n",
    "        \n",
    "        if len(element['labels_raw']) is not 0:\n",
    "            food_to_label.append(element)\n",
    "        #else:\n",
    "        #    print \"Picture\", element['id'], \"has no labels and is being ignored!\"\n",
    "\n",
    "if len(set([element['id'] for element in food_to_label])) != len(food_to_label):\n",
    "    print('something\\'s wrong!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "proportions = []\n",
    "for lb in range(9):\n",
    "    l = len([element for element in food_to_label if lb in element['labels_raw']])/float(len(food_to_label))\n",
    "    print \"Label\", lb, \"is present at\", int(l*100), \"% with respect to all other labels\"\n",
    "    proportions.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data dir\n",
    "imagesDir = './data/SampleFoodClassifier_Norm_100'\n",
    "\n",
    "# Filter out images which might not be present in the folder but are present in the csv file\n",
    "files = [f for f in listdir(imagesDir) if path.isfile(path.join(imagesDir, f))]\n",
    "food_to_label = [element for element in food_to_label if element['id'] in files]\n",
    "\n",
    "del files[:]\n",
    "gc.collect()\n",
    "\n",
    "print \"The new length of the data is\", len(food_to_label)\n",
    "\n",
    "# Parameters\n",
    "test_size = 500\n",
    "learning_rate_start= .001\n",
    "training_size = 30\n",
    "training_iters = 25\n",
    "threshold= 0.1 #threshold for accepting a label (in terms of probability; note: all probabilities sum up to 1)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "sdev= 0.01 #Stddev of the initiliazed variables. Too small causes stationarity of the modell (NOT <0.01!), too large gives large variety\n",
    "\n",
    "seed=ut.resample(np.linspace(1,1000,1000,dtype=int), n_samples=1)[0]\n",
    "print \"Seed used for data split:\", seed\n",
    "\n",
    "# Network Parameters\n",
    "# !! Images: 100x100 RGB = 100, 100, 3\n",
    "w, h, channels = [100, 100, 3]\n",
    "n_classes = 9\n",
    "print \"Width, Height and channels:\", w, h, channels, \". Number of classes:\", n_classes\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, w, h, channels], name=\"x\")\n",
    "y = tf.placeholder(tf.float32, [None, n_classes], name=\"y\")\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    print \"PLEASE MODIFY WD1 TO\", conv2.get_shape().as_list()[1], \"*\",conv2.get_shape().as_list()[2], \"*64\"\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.truncated_normal([5, 5, channels, 32], stddev=sdev, seed=seed), name=\"wc1\"),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=sdev, seed=seed), name=\"wc2\"),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.truncated_normal([25*25*64, 1024], stddev=sdev, seed=seed), name=\"wd1\"),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.truncated_normal([1024, n_classes], stddev=sdev, seed=seed), name=\"outw\")\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.truncated_normal([32], stddev=sdev, seed=seed), name=\"bc1\"),\n",
    "    'bc2': tf.Variable(tf.truncated_normal([64], stddev=sdev, seed=seed), name=\"bc2\"),\n",
    "    'bd1': tf.Variable(tf.truncated_normal([1024], stddev=sdev, seed=seed), name=\"bd1\"),\n",
    "    'out': tf.Variable(tf.truncated_normal([n_classes], stddev=sdev, seed=seed), name=\"outb\")\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(pred, y))\n",
    "\n",
    "#optimizer with adapted learning_rate\n",
    "step = tf.Variable(0, trainable=False, name=\"step\")\n",
    "rate = tf.train.exponential_decay(learning_rate_start, step, 1, 0.9999)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(rate).minimize(cost, global_step=step)\n",
    "\n",
    "# Gives an array of arrays, where each position represents % of belonging to respective classs. Eg: a[0.34, 0.66] --> class 0 : 34%, class 1: 66%\n",
    "classes = tf.nn.softmax(pred)\n",
    "y_p = tf.cast(tf.greater(classes,threshold),dtype=\"float\")\n",
    "\n",
    "#Evaluate model\n",
    "correct_pred = tf.cast(tf.equal(y_p,y),dtype=\"float\")\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "def label_class(x):\n",
    "    for i in range(0,len(x)):\n",
    "        print i, \":\", x[i]\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dataSplit(array, size,seed=seed):\n",
    "    split = [element['id'] for element in ut.shuffle(array, n_samples=size, random_state=seed)]\n",
    "    return [element for element in array if element['id'] in split], [element for element in array if element['id'] not in split]\n",
    "\n",
    "def dataAndLabels(array):\n",
    "    return [np.array(Image.open(path.join(imagesDir, element['id']))) for element in array], [element['labels'] for element in array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get out a sample of data for testing\n",
    "## IMPORTANT: RUN ONLY ONCE!\n",
    "test, food_to_label = dataSplit(food_to_label, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the models\n",
    "saveDir = './tensorflow/all_classes'\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluation(y_pred,y_test, n_classes):\n",
    "    \n",
    "    precision0 = []\n",
    "    precision1 = []\n",
    "    recall0 = []\n",
    "    recall1 = []\n",
    "    F10 = []\n",
    "    F11 = []\n",
    "    tn = []\n",
    "    fn = []\n",
    "    tp = []\n",
    "    fp = []\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        y_pred_i = [float(el[i]) for el in y_pred]\n",
    "        y_test_i = [float(el[i]) for el in y_test]\n",
    "        tn.append(np.sum(np.logical_and(np.equal(y_pred_i,0),np.equal(y_test_i,0))))\n",
    "        fn.append(np.sum(np.logical_and(np.equal(y_pred_i,0),np.equal(y_test_i,1))))\n",
    "        tp.append(np.sum(np.logical_and(np.equal(y_pred_i,1),np.equal(y_test_i,1))))\n",
    "        fp.append(np.sum(np.logical_and(np.equal(y_pred_i,1),np.equal(y_test_i,0))))\n",
    "        try:\n",
    "            precision0.append(float(tn[i])/(fn[i]+tn[i]))\n",
    "        except ZeroDivisionError:\n",
    "            precision0.append(1)\n",
    "        try: \n",
    "            precision1.append(float(tp[i])/(fp[i]+tp[i]))\n",
    "        except ZeroDivisionError:\n",
    "            precision0.append(1)\n",
    "        try:\n",
    "            recall0.append(float(tn[i])/(fp[i]+tn[i]))\n",
    "        except ZeroDivisionError:\n",
    "            recall0.append(1)\n",
    "        try:\n",
    "            recall1.append(float(tp[i])/(tp[i]+fn[i]))\n",
    "        except ZeroDivisionError:\n",
    "            recall1.append(1)\n",
    "        try:\n",
    "            F10.append(2*precision0[i]*recall0[i]/(precision0[i]+recall0[i]))\n",
    "        except ZeroDivisionError:\n",
    "            F10.append(1)\n",
    "        try:\n",
    "            F11.append(2*precision1[i]*recall1[i]/(precision1[i]+recall1[i]))\n",
    "        except:\n",
    "            F11.append(1)\n",
    "    \n",
    "    print\n",
    "        \n",
    "    print 'Precision:'\n",
    "    print \"\\t\" + \"0\" + \"\\t\" + \"\\t\" + \"1\"\n",
    "    print \"---------------------------------------\"\n",
    "    for i in range(n_classes):\n",
    "        print str(i) + \":\"+ \"\\t\" + \"{:.2f}\".format(precision0[i]*100) + \"\\t\" + \"%\\t\" + \"{:.2f}\".format(precision1[i]*100) + \"\\t\"+ \"%\"\n",
    "    print \"---------------------------------------\"\n",
    "    print \"avg:\" + \"\\t\" + \"{:.2f}\".format(100*sum(precision0)/float(len(precision0))) + \"\\t\"+ \"%\\t\" + \"{:.2f}\".format(100*sum(precision1)/float(len(precision1))) + \"\\t\" + \"%\"\n",
    "    \n",
    "    print\n",
    "    print 'Recall:'\n",
    "    print \"\\t\" + \"0\" + \"\\t\" + \"\\t\" + \"1\" \n",
    "    print \"---------------------------------------\"\n",
    "    for i in range(n_classes):\n",
    "        print str(i) + \":\"+ \"\\t\" + \"{:.2f}\".format(100*recall0[i]) + \"\\t\" + \"%\\t\" + \"{:.2f}\".format(100*recall1[i]) + \"\\t\" + \"%\"\n",
    "    print \"---------------------------------------\"\n",
    "    print \"avg:\" + \"\\t\" + \"{:.2f}\".format(100*sum(recall0)/float(len(recall0))) + \"\\t\" + \"%\\t\" + \"{:.2f}\".format(100*sum(recall1)/float(len(recall1))) + \"\\t\" + \"%\"\n",
    "    \n",
    "    print\n",
    "    print 'F1-Score:'\n",
    "    print \"\\t\" + \"0\" + \"\\t\" + \"\\t\" + \"1\" \n",
    "    print \"---------------------------------------\"\n",
    "    for i in range(n_classes):\n",
    "        print str(i) + \":\"+ \"\\t\" + \"{:.2f}\".format(100*F10[i]) + \"\\t\" + \"%\\t\" + \"{:.2f}\".format(100*F11[i]) + \"\\t\" + \"%\"\n",
    "    print \"---------------------------------------\"\n",
    "    print \"avg:\" + \"\\t\" + \"{:.2f}\".format(100*sum(F10)/float(len(F10))) + \"\\t\" + \"%\\t\" + \"{:.2f}\".format(100*sum(F11)/float(len(F11))) + \"\\t\" + \"%\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Keep training until reach max iterations\n",
    "    for epoch in range(training_iters):\n",
    "        if len(food_to_label) < training_size:\n",
    "            del ix[:]\n",
    "            del iy[:]\n",
    "            del batch[:]\n",
    "            break\n",
    "        # Fit training using batch data\n",
    "        print \"Loading batch...\",\n",
    "        batch, food_to_label = dataSplit(food_to_label, training_size)\n",
    "        ix, iy = dataAndLabels(batch)\n",
    "        print \"batch loaded!\"\n",
    "        \n",
    "        print \"Running optimizer...\",\n",
    "        sess.run(optimizer, feed_dict={x: ix, y: iy, keep_prob: 1.})\n",
    "        print \"done!\"\n",
    "        # Compute average loss\n",
    "        #loss, acc = sess.run([cost, accuracy], feed_dict={x: ix, y: iy, keep_prob: 1.})\n",
    "        # Display logs per epoch step\n",
    "        #print \"Iter \" + str(epoch) + \", Minibatch Loss= \" + \\\n",
    "        #          \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "        #          \"{:.5f}\".format(acc)\n",
    "        del ix[:]\n",
    "        del iy[:]\n",
    "        del batch[:]\n",
    "    print \"Optimization Finished!\"\n",
    "    \n",
    "    save_path = saver.save(sess, path.join(saveDir, \"classifier.ckpt\"))\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    runs = 0\n",
    "    acc = 0.\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "    class_pred = []\n",
    "    for i in range(0, test_size, 30): #test batch size = 30\n",
    "        if i+30 < test_size:\n",
    "            x_test, y_test_partial = dataAndLabels(test[i:i+30])\n",
    "        else:\n",
    "            x_test, y_test_partial = dataAndLabels(test[i:])\n",
    "        val_accuracy, y_pred_i, cls = sess.run([accuracy, y_p, classes], feed_dict={x: x_test, y: y_test_partial, keep_prob: 1.})\n",
    "        acc += val_accuracy\n",
    "        y_test.extend(y_test_partial)\n",
    "        y_pred.extend(y_pred_i)\n",
    "        class_pred.extend(cls)\n",
    "        runs += 1\n",
    "        print \"Partial testing accuracy:\", acc/runs\n",
    "    \n",
    "    #metrics\n",
    "    print \"Validation accuracy:\", acc/runs\n",
    "    evaluation(y_test, y_pred, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
