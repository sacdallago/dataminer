{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A Convolutional Network implementation example using TensorFlow library.\n",
    "This example is using the MNIST database of handwritten digits\n",
    "(http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics as mt\n",
    "from sklearn import utils as ut\n",
    "import gc\n",
    "import csv\n",
    "import numpy as np\n",
    "import os.path as path\n",
    "from os import listdir\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dataSplit(array, size):\n",
    "    split = [element['id'] for element in ut.shuffle(array, n_samples=size, random_state=37)]\n",
    "    return [element for element in array if element['id'] in split], [element for element in array if element['id'] not in split]\n",
    "\n",
    "def splitPositiveNegative(array, positiveClass):\n",
    "    return [element for element in array if positiveClass in element['labels_raw']], [element for element in array if positiveClass not in element['labels_raw']]\n",
    "\n",
    "def proportionalDataSplit(positive, negative, positiveSize, size):\n",
    "    positiveSize = np.floor(positiveSize*size).astype(int)\n",
    "    negativeSize = (size-positiveSize).astype(int)\n",
    "    \n",
    "    positiveSplit = [element['id'] for element in ut.shuffle(positive, n_samples=positiveSize, random_state=37)]\n",
    "    negativeSplit = [element['id'] for element in ut.shuffle(negative, n_samples=negativeSize, random_state=37)]\n",
    "    \n",
    "    return [element for element in positive if element['id'] in positiveSplit], [element for element in positive if element['id'] not in positiveSplit], [element for element in negative if element['id'] in negativeSplit], [element for element in negative if element['id'] not in negativeSplit]\n",
    "                     \n",
    "def dataAndLabels(array, negative = None):\n",
    "    if negative is None:\n",
    "        return [np.array(Image.open(path.join(imagesDir, element['id']))) for element in array], [element['labels'] for element in array]\n",
    "    elif type(negative) is type([]):\n",
    "        return [np.array(Image.open(path.join(imagesDir, element['id']))) for element in array] + [np.array(Image.open(path.join(imagesDir, element['id']))) for element in negative], [[0,1]]*len(array) + [[1,0]]*len(negative)\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./photo_to_levels.csv') as f:\n",
    "    food_to_label = []\n",
    "    for row in csv.DictReader(f, skipinitialspace=True):\n",
    "        element = {}\n",
    "        for k, v in row.items():\n",
    "            if k == \"id\":\n",
    "                element['id'] = str(v) + \".jpg\"\n",
    "            elif k == \"labels\":\n",
    "                labels_raw = np.array(str(v).split(' '))\n",
    "                labels = [0] * 9\n",
    "                labels_int = []\n",
    "                try:\n",
    "                    for lb in labels_raw:\n",
    "                        labels[int(str(lb))] = 1\n",
    "                        labels_int.append(int(lb))\n",
    "                except ValueError:\n",
    "                    print \"Failure with value\", lb, \"labels lenght\", len(labels_raw), \"content:\", v\n",
    "                element['labels'] = labels\n",
    "                element['labels_raw'] = labels_int\n",
    "            else :\n",
    "                print \"No idea what you just passed!\"\n",
    "        \n",
    "        if len(element['labels_raw']) is not 0:\n",
    "            food_to_label.append(element)\n",
    "        else:\n",
    "            print \"Picture\", element['id'], \"has no labels and is being ignored!\"\n",
    "\n",
    "if len(set([element['id'] for element in food_to_label])) != len(food_to_label):\n",
    "    print('something\\'s wrong!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "proportions = []\n",
    "for lb in range(9):\n",
    "    l = len([element for element in food_to_label if lb in element['labels_raw']])/float(len(food_to_label))\n",
    "    print \"Label\", lb, \"is present at\", int(l*100), \"% with respect to all other labels\"\n",
    "    proportions.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data dir\n",
    "imagesDir = './data/SampleFoodClassifier_Norm_100'\n",
    "\n",
    "# Filter out images which might not be present in the folder but are present in the csv file\n",
    "files = [f for f in listdir(imagesDir) if path.isfile(path.join(imagesDir, f))]\n",
    "food_to_label = [element for element in food_to_label if element['id'] in files]\n",
    "\n",
    "del files[:]\n",
    "gc.collect()\n",
    "\n",
    "print \"The new length of the data is\", len(food_to_label)\n",
    "\n",
    "# Parameters\n",
    "test_size = 500\n",
    "learning_rate_start= .001\n",
    "training_size = 100\n",
    "training_iters = 100\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# Network Parameters\n",
    "# !! Images: 100x100 RGB = 100, 100, 3\n",
    "w, h, channels = [100, 100, 3]\n",
    "n_classes = 2\n",
    "print \"Width, Height and channels:\", w, h, channels, \". Number of classes:\", n_classes\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, w, h, channels])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    print \"PLEASE MODIFY WD1 TO\", conv2.get_shape().as_list()[1], \"*\",conv2.get_shape().as_list()[2], \"*64\"\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    # fc1 = tf.nn.dropout(fc1, dropout)\n",
    "    \n",
    "    fc2 = tf.add(tf.matmul(fc1, weights['wd2']), biases['bd2'])\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    # Apply Dropout\n",
    "    fc2 = tf.nn.dropout(fc2, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc2, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "sdev= 0.01\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.truncated_normal([5, 5, channels, 32], stddev=sdev)),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=sdev)),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.truncated_normal([25*25*64, 3000], stddev=sdev)),\n",
    "    'wd2': tf.Variable(tf.truncated_normal([3000, 1024], stddev=sdev)),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.truncated_normal([1024, n_classes], stddev=sdev))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.truncated_normal([32], stddev=sdev)),\n",
    "    'bc2': tf.Variable(tf.truncated_normal([64], stddev=sdev)),\n",
    "    'bd1': tf.Variable(tf.truncated_normal([3000], stddev=sdev)),\n",
    "    'bd2': tf.Variable(tf.truncated_normal([1024], stddev=sdev)),\n",
    "    'out': tf.Variable(tf.truncated_normal([n_classes], stddev=sdev))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(pred, y))\n",
    "\n",
    "# optimizer without adapted learning_rate\n",
    "#optimizer = tf.train.AdamOptimizerOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "#optimizer with adapted learning_rate\n",
    "step = tf.Variable(0, trainable=False)\n",
    "rate = tf.train.exponential_decay(learning_rate_start, step, 1, 0.9999)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(rate).minimize(cost, global_step=step)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "y_p = tf.argmax(pred, 1)\n",
    "\n",
    "\n",
    "# Gives an array of arrays, where each position represents % of belonging to respective classs. Eg: a[0.34, 0.66] --> class 0 : 34%, class 1: 66%\n",
    "# classes = tf.nn.softmax(pred)\n",
    "classes = tf.nn.softmax(pred)\n",
    "\n",
    "\n",
    "def label_class(x):\n",
    "    for i in range(0,len(x)):\n",
    "        print i, \":\", x[i]\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the models\n",
    "saveDir = './tensorflow/one_against_all'\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for currentLabel in range(len(proportions)):\n",
    "    print \"Training model for\", currentLabel, \"which is represented by\", proportions[currentLabel]\n",
    "    positive, negative = splitPositiveNegative(food_to_label, currentLabel)\n",
    "    \n",
    "    # Get some test samples\n",
    "    test1, positive, test0, negative = proportionalDataSplit(positive, negative, proportions[currentLabel], test_size)\n",
    "    \n",
    "    positiveSize = np.floor(proportions[currentLabel]*training_size).astype(int)\n",
    "    negativeSize = (training_size-positiveSize).astype(int)\n",
    "    \n",
    "    # Launch the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        # Keep training until reach max iterations\n",
    "        for epoch in range(training_iters):\n",
    "            if len(positive) < positiveSize or len(negative) < negativeSize:\n",
    "                del ix[:]\n",
    "                del iy[:]\n",
    "                del batch1[:]\n",
    "                del batch0[:]\n",
    "                break\n",
    "            # Fit training using batch data\n",
    "            print \"Loading batch...\",\n",
    "            batch1, positive, batch0, negative = proportionalDataSplit(positive, negative, proportions[currentLabel], training_size)\n",
    "            ix, iy = dataAndLabels(batch1, batch0)\n",
    "            print \"bactch loaded!\"\n",
    "\n",
    "            print \"Running optimizer...\",\n",
    "            sess.run(optimizer, feed_dict={x: ix, y: iy, keep_prob: 1.})\n",
    "            print \"done!\"\n",
    "            # Compute average loss\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: ix, y: iy, keep_prob: 1.})\n",
    "            # Display logs per epoch step\n",
    "            print \"Iter \" + str(epoch) + \", Minibatch Loss= \" + \\\n",
    "                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.5f}\".format(acc)\n",
    "            del ix[:]\n",
    "            del iy[:]\n",
    "            del batch1[:]\n",
    "            del batch0[:]\n",
    "        print \"Optimization Finished!\"\n",
    "        \n",
    "        save_path = saver.save(sess, \"/tmp/label-\" + str(currentLabel) + \".ckpt\")\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "        runs = 0\n",
    "        acc = 0.\n",
    "        y_pred = []\n",
    "        class_pred = []\n",
    "        test = test1 + test0\n",
    "        y_test = [[0,1]]*len(test1) + [[1,0]]*len(test0)\n",
    "        for i in range(0, test_size, 30):\n",
    "            if i+30 < test_size:\n",
    "                x_test, _ = dataAndLabels(test[i:i+30])\n",
    "                val_accuracy, y_pred_i, cls = sess.run([accuracy, y_p, classes], feed_dict={x: x_test, y: y_test[i:i+30], keep_prob: 1.})\n",
    "            else:\n",
    "                x_test, _ = dataAndLabels(test[i:])\n",
    "                val_accuracy, y_pred_i, cls = sess.run([accuracy, y_p, classes], feed_dict={x: x_test, y: y_test[i:], keep_prob: 1.})\n",
    "            acc += val_accuracy\n",
    "            y_pred.extend(y_pred_i)\n",
    "            class_pred.extend(cls)\n",
    "            runs += 1\n",
    "            print \"Partial testing accuracy:\", acc/runs\n",
    "\n",
    "        #metrics\n",
    "        print \"Validation accuracy:\", acc/runs\n",
    "        y_true = np.argmax(y_test,1)\n",
    "        print \"Precision for each class:\"\n",
    "        label_class(mt.precision_score(y_true, y_pred, average=None))\n",
    "        print \"Recall for each class:\"\n",
    "        label_class(mt.recall_score(y_true, y_pred, average=None))\n",
    "        print \"F1_score for each class:\"\n",
    "        label_class(mt.f1_score(y_true, y_pred, average=None))\n",
    "        print \"confusion_matrix\"\n",
    "        print mt.confusion_matrix(y_true, y_pred)\n",
    "        fpr, tpr, tresholds = mt.roc_curve(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(y_test)):\n",
    "    print \"For\", i, \"as\", y_test[i]\n",
    "    for j in range(2):\n",
    "        print \"\\t\", j, \"@\", class_pred[i][j]*100\n",
    "    print \"\\n\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
