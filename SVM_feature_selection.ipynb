{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, svm, metrics, cross_validation, grid_search, linear_model, neural_network\n",
    "\n",
    "# Import save and load classifiers utils\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Import data access utils\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Other Tools\n",
    "import re\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadFeatures(folder):\n",
    "    files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "    features = np.array([np.load(os.path.join(folder, f)) for f in files])\n",
    "    simplifiedNames = [re.sub(r'\\.npy*$','',s) for s in files]\n",
    "    return sorted(zip(simplifiedNames, features), key= lambda x : x[0])\n",
    "\n",
    "def createFeaturesDictionary(*features):\n",
    "    # Collect all identifiers\n",
    "    resultingMap = []\n",
    "    featuresDetails = []\n",
    "    \n",
    "    feature = 0\n",
    "    for featureSet in features:\n",
    "        featuresDetails.append({'feature': feature, 'length': len(featureSet[0][1]), 'size': featureSet[0][1].size, 'shape': featureSet[0][1].shape,})\n",
    "        for element in featureSet:\n",
    "            temp = [e for e in resultingMap if e['id'] == element[0]]\n",
    "            if element[1].size != featureSet[0][1].size:\n",
    "                print \"!!!!!!!! FEATURE ARRAY MISMATCH! WILL NEVER WORK!!!!!\"\n",
    "            if len(temp):\n",
    "                temp[0][str(feature)] = element[1]\n",
    "                temp[0]['flat'] = np.append(temp[0]['flat'], element[1])\n",
    "            else:\n",
    "                resultingMap.append({'id': element[0], str(feature): element[1], 'flat': element[1]})\n",
    "        feature += 1\n",
    "    \n",
    "    return resultingMap, featuresDetails\n",
    "\n",
    "def classifierEstimation(classifier, x_test, y_test):\n",
    "    print\n",
    "    print 'Test set has positive', len([i for i in y_test if i is 1]), 'and negative', len([i for i in y_test if i is 0])\n",
    "    print\n",
    "    print 'Best classifier score'\n",
    "    print\n",
    "    print(metrics.classification_report(y_test, classifier.predict(x_test), target_names=['not-food', 'food']))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge labels with data\n",
    "#for element in fmap:\n",
    "#    try:\n",
    "#        label_index = next(index for (index, d) in enumerate(food_no_food) if d[\"id\"] == element['id'])\n",
    "#        element['label'] = food_no_food[label_index]['is_food']\n",
    "#    except(StopIteration):\n",
    "#        print \"This id was not found:\", element['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runSVM(data, labels):\n",
    "    print data.shape, labels.shape\n",
    "\n",
    "    # split training data in a train set and a test set. The test set will containt 20% of the total\n",
    "    x_train, x_test, y_train, y_test = cross_validation.train_test_split(data, labels, test_size=0.25, random_state=6)\n",
    "    print 'Train set has positive', len([i for i in y_train if i == 1]), 'and negative', len([i for i in y_train if i == 0])\n",
    "\n",
    "    # Define the parameter search space\n",
    "    # parameters = {'kernel': ['linear', 'rbf'], 'C': [1, 10, 100, 1000], 'gamma': [0.01, 0.001, 0.0001]}\n",
    "    parameters = {'kernel': ['linear'], 'C': [1000], 'gamma': [0.01]}\n",
    "    \n",
    "    # Search for the best classifier within the search space and return it\n",
    "    kf = cross_validation.KFold(n=len(x_train), n_folds=5, shuffle=True, random_state=3)\n",
    "    # ss = cross_validation.ShuffleSplit(n=len(x_train), test_size=0.25, n_iter=10, random_state=3)\n",
    "\n",
    "    grid = grid_search.GridSearchCV(svm.SVC(verbose=True), parameters, cv=kf)\n",
    "    grid.fit(x_train, y_train)\n",
    "    \n",
    "    # print 'Best parameters:', grid.best_params_\n",
    "    \n",
    "    classifierEstimation(grid.best_estimator_, x_test, y_test)\n",
    "    \n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def equalSize(*arrays):\n",
    "    size = len(arrays[0])\n",
    "    \n",
    "    for i in range(1,len(arrays)):\n",
    "        if len(arrays[i]) != size:\n",
    "            print \"Faulty is number\", i\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "def tryAllCombinations(labels, *features):\n",
    "    if equalSize(labels, *features):\n",
    "        print \"Arrays pased are OK\"\n",
    "    else:\n",
    "        print \"Arrays passed are NOT OK!\"\n",
    "        return\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        \n",
    "        data = features[i]\n",
    "        \n",
    "        print \"Trying with f\", i\n",
    "        # runSVM(data, labels)\n",
    "        \n",
    "        for j in range(i+1, len(features)):\n",
    "            \n",
    "            data = []\n",
    "            \n",
    "            for iterator in range(len(features[0])):\n",
    "                data.append(np.append(features[i][iterator], features[j][iterator]))\n",
    "        \n",
    "            data = np.array(data)\n",
    "            print \"Trying with f\", i, j\n",
    "            runSVM(data, labels)                \n",
    "            \n",
    "#            for k in range(j+1, i-1):\n",
    "#                data = np.append(data, features[j])\n",
    "        \n",
    "#                print \"Trying with f\", i, j, k\n",
    "#                runSVM(data, labels)\n",
    "\n",
    "def tryAllFeatures(labels, *features):\n",
    "    if equalSize(labels, *features):\n",
    "        print \"Arrays pased are OK\"\n",
    "    else:\n",
    "        print \"Arrays passed are NOT OK!\"\n",
    "        return\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for i in range(len(features[0])):\n",
    "        item = []\n",
    "        for j in range(len(features)):\n",
    "            item.extend(features[j][i])\n",
    "        data.append(item)\n",
    "    \n",
    "        \n",
    "    print \"Trying with all features\"\n",
    "    runSVM(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load training info\n",
    "with open('./sample_food_no_food.csv') as f:\n",
    "    food_no_food = [{k: v for k, v in row.items()}\n",
    "        for row in csv.DictReader(f, skipinitialspace=True)]\n",
    "\n",
    "labels = np.array([x[1] for x in sorted([(k['id'], np.int64(k['is_food'])) for k in food_no_food], key= lambda x : x[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chris = './data/svmFeature'\n",
    "sobel = './data/edge_array_sobel'\n",
    "roberts = './data/edge_array_roberts'\n",
    "watershed = './data/watershed_array'\n",
    "skeleton = './data/array_skeleton'\n",
    "binary = './data/binarization_array'\n",
    "a05 = './data/con_array_0.5'\n",
    "a65 = './data/con_array_0.65'\n",
    "\n",
    "\n",
    "aC = np.array([x[1] for x in loadFeatures(chris)])\n",
    "aSo = np.array([x[1] for x in loadFeatures(sobel)])\n",
    "aR = np.array([x[1] for x in loadFeatures(roberts)])\n",
    "aW = np.array([x[1] for x in loadFeatures(watershed)])\n",
    "aSk = np.array([x[1] for x in loadFeatures(skeleton)])\n",
    "aB = np.array([x[1] for x in loadFeatures(binary)])\n",
    "a05 = np.array([x[1] for x in loadFeatures(a05)])\n",
    "a65 = np.array([x[1] for x in loadFeatures(a65)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrays pased are OK\n",
      "Trying with f 0\n",
      "Trying with f 0 1\n",
      "(1300, 90216) (1300,)\n",
      "Train set has positive 745 and negative 230\n",
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "tryAllFeatures(labels, aC, aSo, aR, aW, aSk, aB, a05, a65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
